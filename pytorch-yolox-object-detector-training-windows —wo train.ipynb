{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562e8e77",
   "metadata": {},
   "source": [
    "## Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install PyTorch with CUDA\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# # Install additional dependencies\n",
    "# !pip install matplotlib pandas pillow torchtnt tqdm\n",
    "\n",
    "# # Install extra dependencies for pandas\n",
    "# !pip install tabulate pyarrow fastparquet\n",
    "\n",
    "# # Install package for creating visually distinct colormaps\n",
    "# !pip install distinctipy\n",
    "\n",
    "# # Install utility packages\n",
    "# !pip install cjm_pandas_utils cjm_psl_utils cjm_pil_utils cjm_pytorch_utils cjm_yolox_pytorch cjm_torchvision_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f835e2b",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd77028-bd1b-488e-9a80-25c6d6f18603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "\n",
    "%watermark -v -m -p pandas,numpy,watermark,torch,torchvision,torchaudio,cjm_yolox_pytorch,cjm_pandas_utils,cjm_pil_utils,cjm_psl_utils,cjm_pytorch_utils,cjm_torchvision_tfms \n",
    "\n",
    "print(\" \")\n",
    "%watermark -u -n -t -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e2e9e-1a4b-47ce-b9c5-8cb864fa364a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "import datetime\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_psl_utils.core import download_file, file_extract, get_source_code\n",
    "from cjm_pil_utils.core import resize_img, get_img_files, stack_imgs\n",
    "from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor\n",
    "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
    "from cjm_torchvision_tfms.core import ResizeMax, PadSquare, CustomRandomIoUCrop\n",
    "\n",
    "# Import YOLOX package\n",
    "from cjm_yolox_pytorch.model import build_model, MODEL_CFGS, NORM_STATS\n",
    "from cjm_yolox_pytorch.utils import generate_output_grids\n",
    "from cjm_yolox_pytorch.loss import YOLOXLoss\n",
    "from cjm_yolox_pytorch.inference import YOLOXInferenceWrapper\n",
    "\n",
    "# Import the distinctipy module\n",
    "from distinctipy import distinctipy\n",
    "\n",
    "# Import matplotlib for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Set options for Pandas DataFrame display\n",
    "pd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\n",
    "pd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtnt.utils import get_module_summary\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.tv_tensors import BoundingBoxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.v2  as transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f15e6a-8c59-4d32-a466-571fb9e3d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce96",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd861b2b",
   "metadata": {},
   "source": [
    "### Setting a Random Number Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121746d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.\n",
    "seed = 1234\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59a7a4",
   "metadata": {},
   "source": [
    "### Setting the Device and Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_torch_device()\n",
    "dtype = torch.float32\n",
    "device, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f868",
   "metadata": {},
   "source": [
    "### Setting the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-yolox-object-detector\"\n",
    "\n",
    "# The path for the project folder\n",
    "path=Path('D:/PhytoCleaning/2_PICTURES&CODE/Diseases/DiseaserOrNotTrain/')\n",
    "project_dir = Path(f\"./{path/project_name}/\")\n",
    "# project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store datasets\n",
    "# dataset_dir = Path(\"H:\\Datasets\")\n",
    "dataset_dir = project_dir/'Datasets/'\n",
    "# Create the dataset directory if it does not exist\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store archive files\n",
    "archive_dir = dataset_dir/'../Archive'\n",
    "# Create the archive directory if it does not exist\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir, \n",
    "    \"Dataset Directory:\": dataset_dir, \n",
    "    \"Archive Directory:\": archive_dir\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16f9cb",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1197d58",
   "metadata": {},
   "source": [
    "### Setting the Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the dataset\n",
    "dataset_name = 'hagrid-sample-30k-384p'\n",
    "# dataset_name = 'hagrid-sample-120k-384p'\n",
    "# dataset_name = 'hagrid-sample-250k-384p'\n",
    "# dataset_name = 'hagrid-sample-500k-384p'\n",
    "\n",
    "# Construct the HuggingFace Hub dataset name by combining the username and dataset name\n",
    "hf_dataset = f'cj-mills/{dataset_name}'\n",
    "\n",
    "# Create the path to the zip file that contains the dataset\n",
    "archive_path = Path(f'{archive_dir}/{dataset_name}.zip')\n",
    "\n",
    "# Create the path to the directory where the dataset will be extracted\n",
    "dataset_path = Path(f'{dataset_dir}/{dataset_name}')\n",
    "\n",
    "pd.Series({\n",
    "    \"HuggingFace Dataset:\": hf_dataset, \n",
    "    \"Archive Path:\": archive_path, \n",
    "    \"Dataset Path:\": dataset_path\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d948a",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the HuggingFace Hub dataset URL\n",
    "dataset_url = f\"https://huggingface.co/datasets/{hf_dataset}/resolve/main/{dataset_name}.zip\"\n",
    "print(f\"HuggingFace Dataset URL: {dataset_url}\")\n",
    "\n",
    "# Set whether to delete the archive file after extracting the dataset\n",
    "delete_archive = True\n",
    "\n",
    "# Download the dataset if not present\n",
    "if dataset_path.is_dir():\n",
    "    print(\"Dataset folder already exists\")\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_file(dataset_url, archive_dir)    \n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    file_extract(fname=archive_path, dest=dataset_dir)\n",
    "    \n",
    "    # Delete the archive if specified\n",
    "    if delete_archive: archive_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed702c4",
   "metadata": {},
   "source": [
    "### Getting the Image and Annotation Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the items in the 'dataset_path' directory\n",
    "dir_content = [item for item in dataset_path.iterdir() if item.is_dir()]\n",
    "\n",
    "# Get the path of the 'ann_train_val' directory\n",
    "annotation_dir = dataset_path/'ann_train_val'\n",
    "\n",
    "# Remove the 'ann_train_val' directory from the list of items\n",
    "dir_content.remove(annotation_dir)\n",
    "\n",
    "# Get the path of the remaining directory, which is assumed to be the image directory\n",
    "img_dir = dir_content[0]\n",
    "\n",
    "# Print the paths of the annotation and image directories\n",
    "print(f\"Annotation Directory: {annotation_dir}\")\n",
    "print(f\"Image Directory: {img_dir}\")\n",
    "\n",
    "# Get a list of files in the 'annotation_dir' directory\n",
    "annotation_file_paths = list(annotation_dir.glob('*.json'))\n",
    "\n",
    "# Get a list of folders in the 'img_dir' directory\n",
    "img_folder_paths = [folder for folder in img_dir.iterdir() if folder.is_dir()]\n",
    "\n",
    "# Display the names of the folders using a Pandas DataFrame\n",
    "pd.DataFrame({\"Image Folder\": [folder.name for folder in img_folder_paths], \n",
    "              \"Annotation File\":[file.name for file in annotation_file_paths]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ad68f",
   "metadata": {},
   "source": [
    "### Get Image File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95114dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image files in the 'img_dir' directory\n",
    "img_dict = {\n",
    "    file.stem : file # Create a dictionary that maps file names to file paths\n",
    "    for folder in img_folder_paths # Iterate through each image folder\n",
    "    for file in get_img_files(folder) # Get a list of image files in each image folder\n",
    "}\n",
    "\n",
    "# Print the number of image files\n",
    "print(f\"Number of Images: {len(img_dict)}\")\n",
    "\n",
    "# Display the first five entries from the dictionary using a Pandas DataFrame\n",
    "pd.DataFrame.from_dict(img_dict, orient='index').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b569a",
   "metadata": {},
   "source": [
    "### Get Bounding Box Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0deb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the annotations DataFrame\n",
    "annotation_df_pq_path = dataset_path/'annotations_df.parquet'\n",
    "\n",
    "if annotation_df_pq_path.is_file():\n",
    "    # Load the annotations DataFrame if present\n",
    "    annotation_df = pd.read_parquet(annotation_df_pq_path)\n",
    "else:\n",
    "    # Create a generator that yields Pandas DataFrames containing the data from each JSON file\n",
    "    cls_dataframes = (pd.read_json(f).transpose() for f in tqdm(annotation_file_paths))\n",
    "\n",
    "    # Concatenate the DataFrames into a single DataFrame\n",
    "    annotation_df = pd.concat(cls_dataframes, ignore_index=False)\n",
    "\n",
    "    # Keep only the rows that correspond to the filenames in the 'img_dict' dictionary\n",
    "    annotation_df = annotation_df.loc[list(img_dict.keys())]\n",
    "\n",
    "    # Save the annotations DataFrame to disk\n",
    "    annotation_df.to_parquet(annotation_df_pq_path)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame\n",
    "annotation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35d0e2-3fb5-4432-8aac-4f15144659fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_df.loc[list(img_dict.keys())]\n",
    "list(img_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2aafb",
   "metadata": {},
   "source": [
    "### Inspecting the Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b43ff",
   "metadata": {},
   "source": [
    "#### Get image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e98243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique labels in the 'annotation_df' DataFrame\n",
    "class_names = annotation_df['labels'].explode().unique().tolist()\n",
    "class_names.sort()\n",
    "\n",
    "# Display labels using a Pandas DataFrame\n",
    "pd.DataFrame(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ff7c9",
   "metadata": {},
   "source": [
    "#### Visualize the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aab0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples for each object class\n",
    "class_counts = annotation_df['labels'].explode().value_counts().sort_index()\n",
    "\n",
    "# Plot the distribution\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Classes')\n",
    "plt.xticks(range(len(class_counts.index)), class_names, rotation=75)  # Set the x-axis tick labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d67262",
   "metadata": {},
   "source": [
    "### Visualizing Bounding Box Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc00ff2",
   "metadata": {},
   "source": [
    "#### Generate a color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of colors with a length equal to the number of labels\n",
    "colors = distinctipy.get_colors(len(class_names))\n",
    "\n",
    "# Make a copy of the color map in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colors]\n",
    "\n",
    "# Generate a color swatch to visualize the color map\n",
    "distinctipy.color_swatch(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7be42",
   "metadata": {},
   "source": [
    "#### Download a font file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe87bc",
   "metadata": {},
   "source": [
    "#### Define the bounding box annotation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bboxes = partial(draw_bounding_boxes, fill=False, width=2, font=font_file, font_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ceb562",
   "metadata": {},
   "source": [
    "### Selecting a Sample Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da780a5",
   "metadata": {},
   "source": [
    "#### Load the sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file ID of the first image file\n",
    "file_id = list(img_dict.keys())[0]\n",
    "\n",
    "# Open the associated image file as a RGB image\n",
    "sample_img = Image.open(img_dict[file_id]).convert('RGB')\n",
    "\n",
    "# Print the dimensions of the image\n",
    "print(f\"Image Dims: {sample_img.size}\")\n",
    "\n",
    "# Show the image\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b6b7a-b540-4270-aa99-331847ebefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5206",
   "metadata": {},
   "source": [
    "#### Inspect the corresponding annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row from the 'annotation_df' DataFrame corresponding to the 'file_id'\n",
    "annotation_df.loc[file_id].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467711f-591c-4c18-a3ea-0fa36a82778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = annotation_df.loc[file_id]['bboxes']\n",
    "bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418746f",
   "metadata": {},
   "source": [
    "#### Annotate sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels and bounding box annotations for the sample image\n",
    "labels = annotation_df.loc[file_id]['labels']\n",
    "bboxes = annotation_df.loc[file_id]['bboxes']\n",
    "\n",
    "# Calculate the bounding boxes in the image size scale\n",
    "width, height = sample_img.size\n",
    "bboxes = [[x*width, y*height, w*width, h*height] for x, y, w, h in bboxes]\n",
    "\n",
    "# Annotate the sample image with labels and bounding boxes\n",
    "annotated_tensor = draw_bboxes(\n",
    "    image=transforms.PILToTensor()(sample_img), \n",
    "    boxes=torchvision.ops.box_convert(torch.Tensor(bboxes), 'xywh', 'xyxy'), \n",
    "    labels=labels, \n",
    "    colors=[int_colors[i] for i in [class_names.index(label) for label in labels]]\n",
    ")\n",
    "\n",
    "tensor_to_pil(annotated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62a38e-8c2a-445d-9f74-a1a6098f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e61c4",
   "metadata": {},
   "source": [
    "## Selecting a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867ae70",
   "metadata": {},
   "source": [
    "### Exploring Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(MODEL_CFGS).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854219b",
   "metadata": {},
   "source": [
    "### Loading the YOLOX-Tiny Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the YOLOX model configuration\n",
    "model_type = 'yolox_tiny'\n",
    "# model_type = 'yolox_s'\n",
    "# model_type = 'yolox_m'\n",
    "# model_type = 'yolox_l'\n",
    "# model_type = 'yolox_x'\n",
    "\n",
    "# Set whether to initialize the model with pretrained weights\n",
    "pretrained = True\n",
    "\n",
    "# Create a YOLOX model with the number of output classes equal to the number of class names\n",
    "model = build_model(model_type, len(class_names), pretrained=pretrained).to(device=device, dtype=dtype)\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "model.device = device\n",
    "model.name = model_type\n",
    "\n",
    "# Get stride values for processing output\n",
    "strides = model.bbox_head.strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b45ac4",
   "metadata": {},
   "source": [
    "### Get Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve normalization statistics (mean and std) specific to the pretrained checkpoints\n",
    "norm_stats = [*NORM_STATS[model_type].values()] if pretrained else ([0.5]*3, [1.0]*3)\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "pd.DataFrame(norm_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2592ebb",
   "metadata": {},
   "source": [
    "### Summarizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input to the model\n",
    "backbone_inp = torch.randn(1, 3, 256, 256).to(device)\n",
    "with torch.no_grad(): neck_inp = model.backbone(backbone_inp)\n",
    "with torch.no_grad(): head_inp = model.neck(neck_inp)\n",
    "\n",
    "# Get a summary of the model as a Pandas DataFrame\n",
    "backbone_summary = markdown_to_pandas(f\"{get_module_summary(model.backbone, [backbone_inp])}\").iloc[0]\n",
    "neck_summary = markdown_to_pandas(f\"{get_module_summary(model.neck, [neck_inp])}\").iloc[0]\n",
    "head_summary = markdown_to_pandas(f\"{get_module_summary(model.bbox_head, [head_inp])}\").iloc[0]\n",
    "summary_df = pd.concat([backbone_summary, neck_summary, head_summary], axis=1).transpose()\n",
    "\n",
    "parameters_df = summary_df['# Parameters'].apply(convert_to_numeric)\n",
    "trainable_parameters = summary_df['# Trainable Parameters'].apply(convert_to_numeric)\n",
    "size_df = summary_df['Size (bytes)'].apply(convert_to_numeric)\n",
    "forward_flops_df = summary_df['Forward FLOPs'].apply(convert_to_numeric)\n",
    "\n",
    "# compute sums and add a new row\n",
    "total_row = {'Type': f'{model_type}',\n",
    "             '# Parameters': convert_to_string(parameters_df.sum()),\n",
    "             '# Trainable Parameters': convert_to_string(trainable_parameters.sum()),\n",
    "             'Size (bytes)': convert_to_string(size_df.sum()),\n",
    "             'Forward FLOPs': convert_to_string(forward_flops_df.sum()), \n",
    "             'In size': backbone_summary['In size'], \n",
    "             'Out size': head_summary['Out size']}\n",
    "\n",
    "summary_df = pd.concat([pd.DataFrame([total_row]), summary_df], ignore_index=True)\n",
    "\n",
    "# Remove extra columns\n",
    "summary_df.drop(['In size', 'Out size', 'Contains Uninitialized Parameters?'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf507ad",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cee21e",
   "metadata": {},
   "source": [
    "### Training-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a447ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image IDs\n",
    "img_keys = list(img_dict.keys())\n",
    "\n",
    "# Shuffle the image IDs\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "# Define the percentage of the images that should be used for training\n",
    "train_pct = 0.9\n",
    "val_pct = 0.1\n",
    "\n",
    "# Calculate the index at which to split the subset of image paths into training and validation sets\n",
    "train_split = int(len(img_keys)*train_pct)\n",
    "val_split = int(len(img_keys)*(train_pct+val_pct))\n",
    "\n",
    "# Split the subset of image paths into training and validation sets\n",
    "train_keys = img_keys[:train_split]\n",
    "val_keys = img_keys[train_split:]\n",
    "\n",
    "# Print the number of images in the training and validation sets\n",
    "pd.Series({\n",
    "    \"Training Samples:\": len(train_keys),\n",
    "    \"Validation Samples:\": len(val_keys)\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec00c93",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c6768",
   "metadata": {},
   "source": [
    "#### Set training image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a list of potential input resolutions\n",
    "print([max(strides)*i for i in range(7,21)])\n",
    "\n",
    "# Set training image size to a multiple of the max stride value\n",
    "train_sz = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577b340",
   "metadata": {},
   "source": [
    "#### Initialize the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomIoUCrop object\n",
    "iou_crop = CustomRandomIoUCrop(min_scale=0.3, \n",
    "                               max_scale=1.0, \n",
    "                               min_aspect_ratio=0.5, \n",
    "                               max_aspect_ratio=2.0, \n",
    "                               sampler_options=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "                               trials=400, \n",
    "                               jitter_factor=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f55d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `ResizeMax` object\n",
    "resize_max = ResizeMax(max_sz=train_sz)\n",
    "\n",
    "# Create a `PadSquare` object\n",
    "pad_square = PadSquare(shift=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd376c97",
   "metadata": {},
   "source": [
    "#### Test the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27befa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare bounding box targets\n",
    "targets = {'boxes': BoundingBoxes(torchvision.ops.box_convert(torch.Tensor(bboxes), 'xywh', 'xyxy'), \n",
    "                                format='xyxy', \n",
    "                                canvas_size=sample_img.size[::-1]), \n",
    "           'labels': labels}\n",
    "\n",
    "# Crop the image\n",
    "cropped_img, targets = iou_crop(sample_img, targets)\n",
    "\n",
    "# Resize the image\n",
    "resized_img, targets = resize_max(cropped_img, targets)\n",
    "\n",
    "# Pad the image\n",
    "padded_img, targets = pad_square(resized_img, targets)\n",
    "\n",
    "# Ensure the padded image is the target size\n",
    "resize = transforms.Resize([train_sz] * 2, antialias=True)\n",
    "resized_padded_img, targets = resize(padded_img, targets)\n",
    "\n",
    "# Annotate the augmented image with updated labels and bounding boxes\n",
    "annotated_tensor = draw_bboxes(\n",
    "    image=transforms.PILToTensor()(resized_padded_img), \n",
    "    boxes=targets['boxes'], \n",
    "    labels=targets['labels'], \n",
    "    colors=[int_colors[i] for i in [class_names.index(label) for label in labels]]\n",
    ")\n",
    "\n",
    "# Display the annotated image\n",
    "display(tensor_to_pil(annotated_tensor))\n",
    "\n",
    "pd.Series({\n",
    "    \"Source Image:\": sample_img.size,\n",
    "    \"Cropped Image:\": cropped_img.size,\n",
    "    \"Resized Image:\": resized_img.size,\n",
    "    \"Padded Image:\": padded_img.size,\n",
    "    \"Resized Padded Image:\": resized_padded_img.size,\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa913d1",
   "metadata": {},
   "source": [
    "### Training Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60024b-a1fa-45c5-a174-a5510e65b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from windows_utils import HagridDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b712960",
   "metadata": {},
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose transforms for data augmentation\n",
    "data_aug_tfms = transforms.Compose(\n",
    "    transforms=[\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomZoomOut(fill= (123, 117, 104), side_range=(1.125, 1.5)), \n",
    "            iou_crop\n",
    "        ], p=[0.3, 0.7]),\n",
    "        transforms.ColorJitter(\n",
    "                brightness = (0.875, 1.125),\n",
    "                contrast = (0.5, 1.5),\n",
    "                saturation = (0.5, 1.5),\n",
    "                hue = (-0.05, 0.05),\n",
    "        ),\n",
    "        transforms.RandomGrayscale(),\n",
    "        transforms.RandomEqualize(),\n",
    "        transforms.RandomPosterize(bits=3, p=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compose transforms to resize and pad input images\n",
    "resize_pad_tfm = transforms.Compose([\n",
    "    resize_max, \n",
    "    pad_square,\n",
    "    transforms.Resize([train_sz] * 2, antialias=True)\n",
    "])\n",
    "\n",
    "# Compose transforms to sanitize bounding boxes and normalize input data\n",
    "final_tfms = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.SanitizeBoundingBoxes(),\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "# Define the transformations for training and validation datasets\n",
    "train_tfms = transforms.Compose([\n",
    "    data_aug_tfms, \n",
    "    resize_pad_tfm, \n",
    "    final_tfms\n",
    "])\n",
    "valid_tfms = transforms.Compose([resize_pad_tfm, final_tfms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5269f2",
   "metadata": {},
   "source": [
    "### Initialize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to class indices\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Instantiate the datasets using the defined transformations\n",
    "train_dataset = HagridDataset(train_keys, annotation_df, img_dict, class_to_idx, train_tfms)\n",
    "valid_dataset = HagridDataset(val_keys, annotation_df, img_dict, class_to_idx, valid_tfms)\n",
    "\n",
    "# Print the number of samples in the training and validation datasets\n",
    "pd.Series({\n",
    "    'Training dataset size:': len(train_dataset),\n",
    "    'Validation dataset size:': len(valid_dataset)}\n",
    ").to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07243bd",
   "metadata": {},
   "source": [
    "### Inspect Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f559c8",
   "metadata": {},
   "source": [
    "#### Inspect training set sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc291a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_sample = train_dataset[0]\n",
    "\n",
    "annotated_tensor = draw_bboxes(\n",
    "    image=(denorm_img_tensor(dataset_sample[0], *norm_stats)*255).to(dtype=torch.uint8), \n",
    "    boxes=dataset_sample[1]['boxes'], \n",
    "    labels=[class_names[int(i.item())] for i in dataset_sample[1]['labels']], \n",
    "    colors=[int_colors[int(i.item())] for i in dataset_sample[1]['labels']]\n",
    ")\n",
    "\n",
    "tensor_to_pil(annotated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f94b94-d7fd-4638-8c82-0541f5d042cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0559a0e",
   "metadata": {},
   "source": [
    "#### Inspect validation set sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = valid_dataset[0]\n",
    "\n",
    "annotated_tensor = draw_bboxes(\n",
    "    image=(denorm_img_tensor(dataset_sample[0], *norm_stats)*255).to(dtype=torch.uint8), \n",
    "    boxes=dataset_sample[1]['boxes'], \n",
    "    labels=[class_names[int(i.item())] for i in dataset_sample[1]['labels']], \n",
    "    colors=[int_colors[int(i.item())] for i in dataset_sample[1]['labels']]\n",
    ")\n",
    "\n",
    "tensor_to_pil(annotated_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb8674",
   "metadata": {},
   "source": [
    "### Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ab06a-286d-46c3-b89c-fb4ca2b93593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from windows_utils import tuple_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
